{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVHN.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4De24lgNj7Jk"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "es5E9Qq_j8ON"
      },
      "source": [
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import tarfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIDzg4MksG_J"
      },
      "source": [
        "!wget http://ufldl.stanford.edu/housenumbers/train.tar.gz\n",
        "!wget http://ufldl.stanford.edu/housenumbers/test.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4USCpclOsLlv"
      },
      "source": [
        "train_tar = tarfile.open('train.tar.gz', 'r:gz')\n",
        "train_tar.extractall(os.path.join(os.getcwd(),'data'))\n",
        "\n",
        "test_tar = tarfile.open('test.tar.gz', 'r:gz')\n",
        "test_tar.extractall(os.path.join(os.getcwd(),'data'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvO3TXppsc2q"
      },
      "source": [
        "\n",
        "\n",
        "def showimg(index):\n",
        "    img = cv2.imread('data/train/%d.png'%(index+1))\n",
        "    print(img.shape)\n",
        "    plt.imshow(img[:,:,::-1])\n",
        "\n",
        "def get_box_data(data, index):\n",
        "    meta_data = dict()\n",
        "    meta_data['height'] = []\n",
        "    meta_data['label'] = []\n",
        "    meta_data['left'] = []\n",
        "    meta_data['top'] = []\n",
        "    meta_data['width'] = []\n",
        "\n",
        "    def print_attrs(name, obj):\n",
        "        vals = []\n",
        "        if obj.shape[0] == 1:\n",
        "            vals.append(obj[0][0])\n",
        "        else:\n",
        "            for k in range(obj.shape[0]):\n",
        "                vals.append(int(data[obj[k][0]][0][0]))\n",
        "        meta_data[name] = vals\n",
        "\n",
        "    box = data['/digitStruct/bbox'][index]\n",
        "    # print(box[0][0])\n",
        "    data[box[0]].visititems(print_attrs)\n",
        "    return meta_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJ9WM5pK0xL0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KGwU-yrwIOP"
      },
      "source": [
        "d = h5py.File('data/train/digitStruct.mat', 'r')\n",
        "print(d.keys())\n",
        "a = 234\n",
        "showimg(a)\n",
        "print(get_box_data(d, a))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlS6U5rf1gtk"
      },
      "source": [
        "\n",
        "def create_dataset(dataset):\n",
        "    data = h5py.File('%s/digitStruct.mat'%dataset, 'r')\n",
        "    df = []\n",
        "    for i in tqdm(range(len(data['/digitStruct/name']))):\n",
        "        meta_data = get_box_data(data, i)\n",
        "        num_length = len(meta_data['label'])\n",
        "        if num_length < 6:\n",
        "            dd = {'filename': '%s/%d.png'%(dataset, i+1), 'len': num_length}\n",
        "            for i in range(5):\n",
        "                dd['num%d'%(i+1)] = -1\n",
        "                dd['bbox%d'%(i+1)] = (0, 0, 0, 0)\n",
        "            for i in range(num_length):\n",
        "                dd['num%d'%(i+1)] = int(meta_data['label'][i])\n",
        "                dd['bbox%d'%(i+1)] = (meta_data['left'][i], meta_data['top'][i], meta_data['width'][i], meta_data['height'][i])\n",
        "        df.append(dd)\n",
        "    df = pd.DataFrame(df)\n",
        "    for i in range(1, 6):\n",
        "        df.at[df[df['num%d'%i] == 10].index, 'num%d'%i]=0\n",
        "    for i in range(1, 6):\n",
        "        df.at[df['num%d'%i].isnull(), 'num%d'%i]= 10\n",
        "    for i in range(1, 6):\n",
        "        for j in df['bbox%d'%i][df['bbox%d'%i].isnull()].index:\n",
        "            df.at[j, 'bbox%d'%(i+1)]= (0,0,0,0)\n",
        "    \n",
        "    df = df.dropna()\n",
        "    df = df.reset_index(drop=True)\n",
        "    \n",
        "    df.to_csv('%s.csv'%dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vON-Tvpu172s"
      },
      "source": [
        "create_dataset('data/train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rgOdd7xGNaj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11qgvmj6Dtdi"
      },
      "source": [
        "create_dataset('data/test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlmgc1jQskt0"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSA_iVPsJUDO"
      },
      "source": [
        "import torch\n",
        "from  torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import tarfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwjmgCuDN5EX"
      },
      "source": [
        "class SVHNDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, csv_file, transform=None):\n",
        "        self.data_csv = pd.read_csv(\"data/\"+csv_file)\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data_csv)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.data_csv['filename'][idx]\n",
        "        image = Image.open(img_name) #image=Image.open(img_name)\n",
        "        \n",
        "        height=[]\n",
        "        width=[]\n",
        "        top=[]\n",
        "        left=[]\n",
        "\n",
        "        for i in range(5):\n",
        "            bb = self.data_csv['bbox%d'%(i+1)][idx]\n",
        "            bb = bb[1:(len(bb)-1)]\n",
        "            bb = bb.split(',')\n",
        "            bb = [float(element) for element in bb]\n",
        "            height.append(bb[3])\n",
        "            width.append(bb[2])\n",
        "            top.append(bb[1])\n",
        "            left.append(bb[0])\n",
        "            \n",
        "        new_left = [ii for ii in left if ii != 0]\n",
        "        new_top = [ii for ii in top if ii != 0]\n",
        "        \n",
        "        if new_left==[]:\n",
        "            new_left.append(0)\n",
        "        \n",
        "        if new_top==[]:\n",
        "            new_top.append(0)\n",
        "            \n",
        "        _left = int(min(new_left))\n",
        "        upper = int(min(new_top))\n",
        "        right = int(max(left)) + int(max(width))\n",
        "        lower = int(max(top)) + int(max(height))\n",
        "        \n",
        "        _image = image.crop(box=(_left, upper, right, lower))\n",
        "        #_image = image.crop(box=(52,9,107,55))\n",
        "        \n",
        "        labels=[]\n",
        "        labels.append(int(self.data_csv['len'][idx]))\n",
        "        \n",
        "        for i in range(5):\n",
        "            num = self.data_csv['num%d'%(i+1)][idx]\n",
        "            labels.append(int(num)+1)\n",
        "            \n",
        "        #resize and convert to np array\n",
        "        _image = _image.resize([64,64])\n",
        "        image_array = np.array(_image)\n",
        "        #_image = image_array.reshape([64, 64, 3])\n",
        "        image_array = Image.fromarray(image_array) #image_array = np.array(_image)\n",
        "        labels_array = np.array(labels)\n",
        "        labels_array = torch.from_numpy(labels_array).long()\n",
        "        #labels_array = labels_array.type(torch.LongTensor)\n",
        "        \n",
        "        sample = {'image':image_array, 'labels': labels_array}\n",
        "        \n",
        "        if self.transform:\n",
        "            sample['image'] = self.transform(sample['image'])\n",
        "            #sample['labels'] = self.transform(sample['labels'])\n",
        "            #sample = self.transform(sample)\n",
        "            \n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXZ2aHUKN_eQ"
      },
      "source": [
        "test_dataset = SVHNDataset('test.csv')\n",
        "test_sample = test_dataset.__getitem__(0)\n",
        "plt.imshow(test_sample['image'])\n",
        "print(test_sample['labels'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnvhVF3qTYbr"
      },
      "source": [
        "test_dataset = SVHNDataset('train.csv')\n",
        "test_sample = test_dataset.__getitem__(0)\n",
        "plt.imshow(test_sample['image'])\n",
        "print(test_sample['labels'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5dDNG_WOC8B"
      },
      "source": [
        "train_transforms= transforms.Compose([     \n",
        "    #transforms.RandonCrop(54)        \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "            ])\n",
        "\n",
        "test_transforms= transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "            ])\n",
        "\n",
        "train_dataset = SVHNDataset('train.csv', transform=train_transforms)\n",
        "test_dataset = SVHNDataset('test.csv', transform=test_transforms)\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size = 64, shuffle = True)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size = 64, shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAhe83C3Tq3O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18tyNPhKT0t4"
      },
      "source": [
        "Defining Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BXH-LkOT44P"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        hidden1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=48, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(num_features=48),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "        hidden2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=48, out_channels=64, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(num_features=64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=1, padding=1),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "        hidden3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "        hidden4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=128, out_channels=160, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(num_features=160),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=1, padding=1),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "        hidden5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=160, out_channels=192, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(num_features=192),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "        hidden6 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=192, out_channels=192, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(num_features=192),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=1, padding=1),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "        hidden7 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=192, out_channels=192, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(num_features=192),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "        hidden8 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=192, out_channels=192, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(num_features=192),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=1, padding=1),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "        hidden9 = nn.Sequential(\n",
        "            nn.Linear(192 * 7 * 7, 3072),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        hidden10 = nn.Sequential(\n",
        "            nn.Linear(3072, 3072),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self._features = nn.Sequential(\n",
        "            hidden1,\n",
        "            hidden2,\n",
        "            hidden3,\n",
        "            hidden4,\n",
        "            hidden5,\n",
        "            hidden6,\n",
        "            hidden7,\n",
        "            hidden8\n",
        "        )\n",
        "        self._classifier = nn.Sequential(\n",
        "            hidden9,\n",
        "            hidden10\n",
        "        )\n",
        "        self._digit_length = nn.Sequential(nn.Linear(3072, 7))\n",
        "        self._digit1 = nn.Sequential(nn.Linear(3072, 11))\n",
        "        self._digit2 = nn.Sequential(nn.Linear(3072, 11))\n",
        "        self._digit3 = nn.Sequential(nn.Linear(3072, 11))\n",
        "        self._digit4 = nn.Sequential(nn.Linear(3072, 11))\n",
        "        self._digit5 = nn.Sequential(nn.Linear(3072, 11))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self._features(x)\n",
        "        x = x.view(x.size(0), 192 * 7 * 7)\n",
        "        x = self._classifier(x)\n",
        "\n",
        "        length_logits, digits_logits = self._digit_length(x), [self._digit1(x),\n",
        "                                                               self._digit2(x),\n",
        "                                                               self._digit3(x),\n",
        "                                                               self._digit4(x),\n",
        "                                                               self._digit5(x)]\n",
        "        return length_logits, digits_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKSVojnyT6Wg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7J35y1M4U4k3"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    device = 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hM2HWToSU6Ac"
      },
      "source": [
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7vjj8JMU67e"
      },
      "source": [
        "def _loss(length_logits, digits_logits, labels):\n",
        "    length_cross_entropy = torch.nn.functional.cross_entropy(length_logits, labels[:,0])\n",
        "    digit1_cross_entropy = torch.nn.functional.cross_entropy(digits_logits[0], labels[:,1])\n",
        "    digit2_cross_entropy = torch.nn.functional.cross_entropy(digits_logits[1], labels[:,2])\n",
        "    digit3_cross_entropy = torch.nn.functional.cross_entropy(digits_logits[2], labels[:,3])\n",
        "    digit4_cross_entropy = torch.nn.functional.cross_entropy(digits_logits[3], labels[:,4])\n",
        "    digit5_cross_entropy = torch.nn.functional.cross_entropy(digits_logits[4], labels[:,5])\n",
        "    loss = length_cross_entropy + digit1_cross_entropy + digit2_cross_entropy + digit3_cross_entropy + digit4_cross_entropy + digit5_cross_entropy\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xzhPXPxVuKF"
      },
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=10, verbose=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        \n",
        "        SVHN_model_checkpoint={'model_state_dict':model.state_dict(), \n",
        "                       'optimizer_state_dict': optimizer.state_dict(),\n",
        "                       'train_losses':train_losses,\n",
        "                       'test_losses':test_losses,\n",
        "                       'training_time':training_time,\n",
        "                       'no_of_steps':no_of_steps,\n",
        "                       }\n",
        "        \n",
        "        torch.save(SVHN_model_checkpoint, 'SVHN_model_checkpoint.tar')\n",
        "        self.val_loss_min = val_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iB0ANQQYZet6"
      },
      "source": [
        "!export CUDA_LAUNCH_BLOCKING=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYpfLpQqWhMo"
      },
      "source": [
        "model = Model()\n",
        "model.to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
        "early_stopping = EarlyStopping(patience=10, verbose=True)\n",
        "\n",
        "if os.path.isfile('SVHN_model_checkpoint.tar'):\n",
        "    print('SVHN_model_checkpoint.tar found..')\n",
        "    print('Loading checkpoint..')\n",
        "    checkpoint = torch.load('SVHN_model_checkpoint.tar')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    train_losses = checkpoint['train_losses']\n",
        "    test_losses = checkpoint['test_losses']\n",
        "    training_time = checkpoint['training_time']\n",
        "    no_of_steps = checkpoint['no_of_steps']\n",
        "    print('Finished loading checkpoint..')\n",
        "else:\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    training_time = 0\n",
        "    no_of_steps = 0\n",
        "    \n",
        "\n",
        "running_loss = 0\n",
        "accuracy = 0\n",
        "print_every = 50\n",
        "test_loss_min = np.Inf\n",
        "\n",
        "while True:\n",
        "    start = time.time()\n",
        "    #loop through the whole tranloader\n",
        "    for sample in trainloader:\n",
        "        no_of_steps += 1\n",
        "        \n",
        "        image = sample['image']\n",
        "        labels = sample['labels']\n",
        "        \n",
        "        image, labels = image.to(device), labels.to(device)\n",
        "        \n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        length_logits, digits_logits = model.forward(image)\n",
        "        # print(\"model forwarded\")\n",
        "        loss = _loss(length_logits, digits_logits, labels)\n",
        "        # print('loss calculated')\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        # print(loss.item())\n",
        "        \n",
        "        if no_of_steps % print_every == 0:\n",
        "            test_loss = 0\n",
        "            accuracy = 0\n",
        "            num_correct = 0\n",
        "            needs_include_length = False\n",
        "            \n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                for sample in testloader:\n",
        "                    \n",
        "                    image = sample['image']\n",
        "                    labels = sample['labels']\n",
        "                    \n",
        "                    image, labels = image.to(device), labels.to(device)\n",
        "                    \n",
        "                    \n",
        "                    length_logits, digits_logits = model.forward(image)\n",
        "                    batch_loss = _loss(length_logits, digits_logits, labels)\n",
        "                                      \n",
        "                    test_loss += batch_loss.item()\n",
        "                    \n",
        "                    \n",
        "                    # Calculate accuracy\n",
        "                                    \n",
        "                    _, length_top_class = length_logits.topk(1, dim=1)\n",
        "                    \n",
        "                    digits_top_class = []\n",
        "                    \n",
        "                    for i in range(5):\n",
        "                      _, _digits_top_class = digits_logits[i].topk(1, dim=1)\n",
        "                      digits_top_class.append(_digits_top_class)\n",
        "                    \n",
        "                    equals = length_top_class == labels[:,0].view(*length_top_class.shape)\n",
        "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "                    \n",
        "                    for i in range(5):\n",
        "                        equals = digits_top_class[i] == labels[:,i+1].view(*digits_top_class[i].shape)\n",
        "                        accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "                    \n",
        "                    # accuracy = torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "                    \n",
        "                   \n",
        "            test_losses.append(test_loss/len(testloader))        \n",
        "            train_losses.append(running_loss/print_every)\n",
        "            training_time += (time.time() - start)\n",
        "            valid_loss = test_loss/len(testloader)\n",
        "            \n",
        "            print(f\"Steps {no_of_steps}.. \"\n",
        "                  f\"Training time(secs) {training_time/60:.3f}.. \"\n",
        "                  f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
        "                  f\"Test loss: {valid_loss:.3f}.. \"\n",
        "                  f\"Test accuracy: {accuracy/(6*len(testloader)):.3f}\")\n",
        "            \n",
        "            early_stopping(valid_loss, model)\n",
        "            \n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "            \n",
        "            running_loss = 0\n",
        "            model.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAWtycg_g7LJ"
      },
      "source": [
        "!export CUDA_LAUNCH_BLOCKING=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prJTrxlXWxg5"
      },
      "source": [
        " "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}